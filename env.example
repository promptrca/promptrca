# PromptRCA Core Environment Configuration
# Copy this file to .env and customize the values

# =============================================================================
# AWS Configuration
# =============================================================================

# AWS Region (required)
AWS_REGION=eu-west-1
# Alternative: AWS_DEFAULT_REGION=eu-west-1

# =============================================================================
# Global Model Configuration (fallback for all agents)
# =============================================================================

# Default Bedrock model for all agents (fallback)
BEDROCK_MODEL_ID=openai.gpt-oss-120b-1:0

# Default temperature for all agents (fallback)
PROMPTRCA_TEMPERATURE=0.7

# Optional: Maximum tokens for model responses
# PROMPTRCA_MAX_TOKENS=4000

# =============================================================================
# Per-Agent Model Configuration
# =============================================================================
# Each agent can use a different model and temperature
# If not specified, agents will use the global defaults above

# Lead Orchestrator Agent (coordinates all other agents)
PROMPTRCA_ORCHESTRATOR_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0
PROMPTRCA_ORCHESTRATOR_TEMPERATURE=0.7

# Specialized AWS Service Agents
PROMPTRCA_LAMBDA_AGENT_MODEL_ID=openai.gpt-oss-120b-1:0
PROMPTRCA_LAMBDA_AGENT_TEMPERATURE=0.5

PROMPTRCA_APIGATEWAY_AGENT_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0
PROMPTRCA_APIGATEWAY_AGENT_TEMPERATURE=0.6

PROMPTRCA_STEPFUNCTIONS_AGENT_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0
PROMPTRCA_STEPFUNCTIONS_AGENT_TEMPERATURE=0.6

PROMPTRCA_IAM_AGENT_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0
PROMPTRCA_IAM_AGENT_TEMPERATURE=0.4

PROMPTRCA_DYNAMODB_AGENT_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0
PROMPTRCA_DYNAMODB_AGENT_TEMPERATURE=0.6

S3_AGENT_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0
S3_AGENT_TEMPERATURE=0.6

PROMPTRCA_SQS_AGENT_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0
PROMPTRCA_SQS_AGENT_TEMPERATURE=0.6

PROMPTRCA_SNS_AGENT_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0
PROMPTRCA_SNS_AGENT_TEMPERATURE=0.6

PROMPTRCA_EVENTBRIDGE_AGENT_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0
PROMPTRCA_EVENTBRIDGE_AGENT_TEMPERATURE=0.6

PROMPTRCA_VPC_AGENT_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0
PROMPTRCA_VPC_AGENT_TEMPERATURE=0.6

# Analysis Agents
PROMPTRCA_HYPOTHESIS_AGENT_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0
PROMPTRCA_HYPOTHESIS_AGENT_TEMPERATURE=0.3

PROMPTRCA_ROOT_CAUSE_AGENT_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0
PROMPTRCA_ROOT_CAUSE_AGENT_TEMPERATURE=0.2

# =============================================================================
# Synthesis Model Configuration
# =============================================================================
# Used for conservative analysis and synthesis tasks

PROMPTRCA_SYNTHESIS_TEMPERATURE=0.2

# =============================================================================
# AWS Knowledge MCP Server Configuration (Optional)
# =============================================================================
# Integration with AWS Knowledge MCP Server for enhanced documentation access

# Enable/disable AWS Knowledge MCP integration (default: false)
ENABLE_AWS_KNOWLEDGE_MCP=false

# AWS Knowledge MCP Server URL (default: https://knowledge-mcp.global.api.aws)
# AWS_KNOWLEDGE_MCP_URL=https://knowledge-mcp.global.api.aws

# Request timeout in seconds (default: 5)
# AWS_KNOWLEDGE_MCP_TIMEOUT=5

# Maximum retry attempts (default: 2)
# AWS_KNOWLEDGE_MCP_RETRIES=2

# =============================================================================
# Example Model Configurations
# =============================================================================

# Example 1: Use Claude Sonnet for orchestrator and analysis, Haiku for specialists
# ORCHESTRATOR_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0
# HYPOTHESIS_AGENT_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0
# ROOT_CAUSE_AGENT_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0
# LAMBDA_AGENT_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0
# APIGATEWAY_AGENT_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0

# Example 2: Use different models for different complexity tasks
# ORCHESTRATOR_MODEL_ID=anthropic.claude-3-5-sonnet-20241022-v2:0
# LAMBDA_AGENT_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0
# IAM_AGENT_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0

# Example 3: Use OpenAI models for all agents
# BEDROCK_MODEL_ID=openai.gpt-4o-2024-05-13
# PROMPTRCA_TEMPERATURE=0.7

# =============================================================================
# OpenTelemetry Observability Configuration
# =============================================================================
# Generic OpenTelemetry setup supporting multiple backends: Langfuse, AWS X-Ray, or any OTLP-compatible backend

# =============================================================================
# Option 1: Langfuse (Self-hosted or Cloud)
# =============================================================================
# Langfuse Project Configuration (get these from Langfuse UI)
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_SECRET_KEY=sk-lf-...
LANGFUSE_HOST=https://cloud.langfuse.com  # ðŸ‡ªðŸ‡º EU region (default)
# LANGFUSE_HOST=https://us.cloud.langfuse.com  # ðŸ‡ºðŸ‡¸ US region

# Langfuse OTLP Configuration
# Basic Auth header is automatically generated from LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY
OTEL_EXPORTER_OTLP_ENDPOINT=https://cloud.langfuse.com/api/public/otel
OTEL_SERVICE_NAME=promptrca-server

# =============================================================================
# Option 2: AWS X-Ray (via OTLP)
# =============================================================================
# Uncomment these lines to use AWS X-Ray instead of Langfuse
# OTEL_EXPORTER_OTLP_ENDPOINT=https://xray.amazonaws.com/v1/traces
# OTEL_SERVICE_NAME=promptrca-server
# AWS_REGION=eu-west-1  # Your AWS region

# =============================================================================
# Option 3: Generic OTLP Backend (Jaeger, Zipkin, etc.)
# =============================================================================
# Uncomment these lines to use any OTLP-compatible backend
# OTEL_EXPORTER_OTLP_ENDPOINT=http://your-otlp-backend:4317/v1/traces
# OTEL_SERVICE_NAME=promptrca-server
# OTEL_EXPORTER_OTLP_HEADERS=Authorization=Bearer your-token,Other-Header=value

# =============================================================================
# Development Options
# =============================================================================
# OTEL_CONSOLE_EXPORT=true  # Also print traces to console for debugging

# =============================================================================
# Notes
# =============================================================================

# 1. Model precedence: Agent-specific > Global > Hardcoded default
# 2. If an agent-specific MODEL_ID is not set, it uses BEDROCK_MODEL_ID
# 3. If an agent-specific TEMPERATURE is not set, it uses PROMPTRCA_TEMPERATURE
# 4. If neither is set, it uses hardcoded defaults
# 5. Temperature values should be between 0.0 and 1.0
# 6. Model IDs must be valid Bedrock model identifiers
# 7. OpenTelemetry traces will be available in your configured backend (Langfuse, X-Ray, etc.)
# 8. Token counts and costs are automatically tracked via OpenTelemetry
# 9. Backend is automatically detected based on endpoint URL and available credentials
# 10. For X-Ray: Ensure AWS credentials are configured (via AWS CLI, IAM roles, or environment variables)
